{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 4 - Reinforcement Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read details of the environment & model from a text file\n",
    "Read: \n",
    "\n",
    "- Width, Height\n",
    "- Noise: probability of not going in the intended direction (then divided by two for two unintentional neighbor directions)\n",
    "- Immediate rewards of non-goal states\n",
    "- Terminal (goal) states: their locations and rewards\n",
    "- Internal Walls: locations\n",
    "\n",
    "Create the transition matrix (T) and the Rewards matrix according to above details.\n",
    "\n",
    "\"grid1.txt\" corresponds to the example in AIMA and the lecture slides. You can double-check various results there.\n",
    "\n",
    "Python style comments in grid1.txt are just comments, can be ignored/deleted."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class='alert alert-block alert-success' style=\"font-weight:bolder\">\n",
    "\n",
    "### Task1\n",
    "    \n",
    "We ask you to create two more grids (files) of moderate size and with at least one internal wall. And perform the analysis described below on these three grids.\n",
    "    \n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[0.9 0.1 0.9 0.1]\n",
      "  [0.  0.8 0.1 0.1]\n",
      "  [0.  0.  0.  0. ]\n",
      "  [0.  0.  0.  0. ]\n",
      "  [0.1 0.1 0.  0.8]\n",
      "  [0.  0.  0.  0. ]\n",
      "  [0.  0.  0.  0. ]\n",
      "  [0.  0.  0.  0. ]\n",
      "  [0.  0.  0.  0. ]\n",
      "  [0.  0.  0.  0. ]\n",
      "  [0.  0.  0.  0. ]\n",
      "  [0.  0.  0.  0. ]]\n",
      "\n",
      " [[0.8 0.  0.1 0.1]\n",
      "  [0.2 0.2 0.8 0.8]\n",
      "  [0.  0.8 0.1 0.1]\n",
      "  [0.  0.  0.  0. ]\n",
      "  [0.  0.  0.  0. ]\n",
      "  [0.  0.  0.  0. ]\n",
      "  [0.  0.  0.  0. ]\n",
      "  [0.  0.  0.  0. ]\n",
      "  [0.  0.  0.  0. ]\n",
      "  [0.  0.  0.  0. ]\n",
      "  [0.  0.  0.  0. ]\n",
      "  [0.  0.  0.  0. ]]\n",
      "\n",
      " [[0.  0.  0.  0. ]\n",
      "  [0.8 0.  0.1 0.1]\n",
      "  [0.1 0.1 0.8 0. ]\n",
      "  [0.  0.8 0.1 0.1]\n",
      "  [0.  0.  0.  0. ]\n",
      "  [0.  0.  0.  0. ]\n",
      "  [0.1 0.1 0.  0.8]\n",
      "  [0.  0.  0.  0. ]\n",
      "  [0.  0.  0.  0. ]\n",
      "  [0.  0.  0.  0. ]\n",
      "  [0.  0.  0.  0. ]\n",
      "  [0.  0.  0.  0. ]]\n",
      "\n",
      " [[0.  0.  0.  0. ]\n",
      "  [0.  0.  0.  0. ]\n",
      "  [0.8 0.  0.1 0.1]\n",
      "  [0.1 0.9 0.9 0.1]\n",
      "  [0.  0.  0.  0. ]\n",
      "  [0.  0.  0.  0. ]\n",
      "  [0.  0.  0.  0. ]\n",
      "  [0.1 0.1 0.  0.8]\n",
      "  [0.  0.  0.  0. ]\n",
      "  [0.  0.  0.  0. ]\n",
      "  [0.  0.  0.  0. ]\n",
      "  [0.  0.  0.  0. ]]\n",
      "\n",
      " [[0.1 0.1 0.8 0. ]\n",
      "  [0.  0.  0.  0. ]\n",
      "  [0.  0.  0.  0. ]\n",
      "  [0.  0.  0.  0. ]\n",
      "  [0.8 0.8 0.2 0.2]\n",
      "  [0.  0.  0.  0. ]\n",
      "  [0.  0.  0.  0. ]\n",
      "  [0.  0.  0.  0. ]\n",
      "  [0.1 0.1 0.  0.8]\n",
      "  [0.  0.  0.  0. ]\n",
      "  [0.  0.  0.  0. ]\n",
      "  [0.  0.  0.  0. ]]\n",
      "\n",
      " [[0.  0.  0.  0. ]\n",
      "  [0.1 0.1 0.8 0. ]\n",
      "  [0.  0.  0.  0. ]\n",
      "  [0.  0.  0.  0. ]\n",
      "  [0.8 0.  0.1 0.1]\n",
      "  [0.  0.  0.  0. ]\n",
      "  [0.  0.8 0.1 0.1]\n",
      "  [0.  0.  0.  0. ]\n",
      "  [0.  0.  0.  0. ]\n",
      "  [0.1 0.1 0.  0.8]\n",
      "  [0.  0.  0.  0. ]\n",
      "  [0.  0.  0.  0. ]]\n",
      "\n",
      " [[0.  0.  0.  0. ]\n",
      "  [0.  0.  0.  0. ]\n",
      "  [0.1 0.1 0.8 0. ]\n",
      "  [0.  0.  0.  0. ]\n",
      "  [0.  0.  0.  0. ]\n",
      "  [0.  0.  0.  0. ]\n",
      "  [0.8 0.  0.1 0.1]\n",
      "  [0.  0.8 0.1 0.1]\n",
      "  [0.  0.  0.  0. ]\n",
      "  [0.  0.  0.  0. ]\n",
      "  [0.1 0.1 0.  0.8]\n",
      "  [0.  0.  0.  0. ]]\n",
      "\n",
      " [[0.  0.  0.  0. ]\n",
      "  [0.  0.  0.  0. ]\n",
      "  [0.  0.  0.  0. ]\n",
      "  [0.1 0.1 0.8 0. ]\n",
      "  [0.  0.  0.  0. ]\n",
      "  [0.  0.  0.  0. ]\n",
      "  [0.8 0.  0.1 0.1]\n",
      "  [0.  0.8 0.1 0.1]\n",
      "  [0.  0.  0.  0. ]\n",
      "  [0.  0.  0.  0. ]\n",
      "  [0.  0.  0.  0. ]\n",
      "  [0.1 0.1 0.  0.8]]\n",
      "\n",
      " [[0.  0.  0.  0. ]\n",
      "  [0.  0.  0.  0. ]\n",
      "  [0.  0.  0.  0. ]\n",
      "  [0.  0.  0.  0. ]\n",
      "  [0.1 0.1 0.8 0. ]\n",
      "  [0.  0.  0.  0. ]\n",
      "  [0.  0.  0.  0. ]\n",
      "  [0.  0.  0.  0. ]\n",
      "  [0.9 0.1 0.1 0.9]\n",
      "  [0.  0.8 0.1 0.1]\n",
      "  [0.  0.  0.  0. ]\n",
      "  [0.  0.  0.  0. ]]\n",
      "\n",
      " [[0.  0.  0.  0. ]\n",
      "  [0.  0.  0.  0. ]\n",
      "  [0.  0.  0.  0. ]\n",
      "  [0.  0.  0.  0. ]\n",
      "  [0.  0.  0.  0. ]\n",
      "  [0.  0.  0.  0. ]\n",
      "  [0.  0.  0.  0. ]\n",
      "  [0.  0.  0.  0. ]\n",
      "  [0.8 0.  0.1 0.1]\n",
      "  [0.2 0.2 0.8 0.8]\n",
      "  [0.  0.8 0.1 0.1]\n",
      "  [0.  0.  0.  0. ]]\n",
      "\n",
      " [[0.  0.  0.  0. ]\n",
      "  [0.  0.  0.  0. ]\n",
      "  [0.  0.  0.  0. ]\n",
      "  [0.  0.  0.  0. ]\n",
      "  [0.  0.  0.  0. ]\n",
      "  [0.  0.  0.  0. ]\n",
      "  [0.1 0.1 0.8 0. ]\n",
      "  [0.  0.  0.  0. ]\n",
      "  [0.  0.  0.  0. ]\n",
      "  [0.8 0.  0.1 0.1]\n",
      "  [0.1 0.1 0.  0.8]\n",
      "  [0.  0.8 0.1 0.1]]\n",
      "\n",
      " [[0.  0.  0.  0. ]\n",
      "  [0.  0.  0.  0. ]\n",
      "  [0.  0.  0.  0. ]\n",
      "  [0.  0.  0.  0. ]\n",
      "  [0.  0.  0.  0. ]\n",
      "  [0.  0.  0.  0. ]\n",
      "  [0.  0.  0.  0. ]\n",
      "  [0.1 0.1 0.8 0. ]\n",
      "  [0.  0.  0.  0. ]\n",
      "  [0.  0.  0.  0. ]\n",
      "  [0.8 0.  0.1 0.1]\n",
      "  [0.1 0.9 0.1 0.9]]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Enter number of grid : grid1, grid2, grid3\n",
    "file1 = open('grid1.txt', 'r')\n",
    "\n",
    "(W, H) = [t(s) for t, s in zip((int, int), file1.readline().split())]\n",
    "noise = [t(s) for t, s in zip((float, str), file1.readline().split())][0]\n",
    "immediate_rewards = [t(s) for t, s in zip((float, str), file1.readline().split())][0]\n",
    "N = W * H  #total number of states\n",
    "NA = 4  #number of actions\n",
    "T = np.zeros((N, N, NA))  #state transition probabilities\n",
    "Directions = np.array(['L', 'R', 'U', 'D'])\n",
    "Rewards = np.zeros((N, 1)) + immediate_rewards  #immediate rewards\n",
    "Terminals = np.zeros((N, 1))  #terminal states\n",
    "Walls = np.zeros((N, 1))  #(internal) wall states\n",
    "prob1 = 1 - noise\n",
    "prob2 = noise / 2\n",
    "while True:\n",
    "    line = file1.readline()\n",
    "    if not line:\n",
    "        break\n",
    "    if line.find('T') != -1:\n",
    "        (_, x, y, r) = [t(s) for t, s in zip((str, int, int, float), line.split())]\n",
    "        Terminals[y * W + x] = 1\n",
    "        Rewards[y * W + x] = r\n",
    "    if line.find('W') != -1:\n",
    "        (_, x, y) = [t(s) for t, s in zip((str, int, int), line.split())]\n",
    "        Walls[y * W + x] = 1\n",
    "file1.close()\n",
    "for i in range(W):\n",
    "    for j in range(H):\n",
    "        if i + 1 < W and Walls[j * W + i + 1] == 0:  #not wall on the right\n",
    "            T[j * W + i, j * W + i + 1, 1] += prob1\n",
    "            T[j * W + i, j * W + i + 1, 2] += prob2\n",
    "            T[j * W + i, j * W + i + 1, 3] += prob2\n",
    "        else:\n",
    "            T[j * W + i, j * W + i, 1] += prob1\n",
    "            T[j * W + i, j * W + i, 2] += prob2\n",
    "            T[j * W + i, j * W + i, 3] += prob2\n",
    "        if i - 1 >= 0 and Walls[j * W + i - 1] == 0:  #not wall on the left\n",
    "            T[j * W + i, j * W + i - 1, 0] += prob1\n",
    "            T[j * W + i, j * W + i - 1, 2] += prob2\n",
    "            T[j * W + i, j * W + i - 1, 3] += prob2\n",
    "        else:\n",
    "            T[j * W + i, j * W + i, 0] += prob1\n",
    "            T[j * W + i, j * W + i, 2] += prob2\n",
    "            T[j * W + i, j * W + i, 3] += prob2\n",
    "        if j + 1 < H and Walls[(j + 1) * W + i] == 0:  #not wall below\n",
    "            T[j * W + i, (j + 1) * W + i, 3] += prob1\n",
    "            T[j * W + i, (j + 1) * W + i, 0] += prob2\n",
    "            T[j * W + i, (j + 1) * W + i, 1] += prob2\n",
    "        else:\n",
    "            T[j * W + i, j * W + i, 3] += prob1\n",
    "            T[j * W + i, j * W + i, 0] += prob2\n",
    "            T[j * W + i, j * W + i, 1] += prob2\n",
    "        if j - 1 >= 0 and Walls[(j - 1) * W + i] == 0:  #not wall above\n",
    "            T[j * W + i, (j - 1) * W + i, 2] += prob1\n",
    "            T[j * W + i, (j - 1) * W + i, 0] += prob2\n",
    "            T[j * W + i, (j - 1) * W + i, 1] += prob2\n",
    "        else:\n",
    "            T[j * W + i, j * W + i, 2] += prob1\n",
    "            T[j * W + i, j * W + i, 0] += prob2\n",
    "            T[j * W + i, j * W + i, 1] += prob2\n",
    "\n",
    "print(T)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class='alert alert-block alert-success' style=\"font-weight:bolder\">\n",
    "\n",
    "## Task 2 - Implement functions\n",
    "\n",
    "### Task 2a - Value and Policy iteration\n",
    "Value iteration is provided, implement policy iteration\n",
    "\n",
    "### Task 2b - TD-Learning and Q-Learning\n",
    "Q-Learning is implemented, implement TD-Learning\n",
    "    \n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "def value_iteration(gamma, T, Rewards, Terminals, Walls):\n",
    "    N, NA = T.shape[0], T.shape[2]\n",
    "    V = np.zeros((N, 1))\n",
    "    Policy = np.zeros((N, 1))\n",
    "    iterations = 0\n",
    "    while True:\n",
    "        iterations += 1\n",
    "\n",
    "        V_old = np.copy(V)\n",
    "        for s in range(N):\n",
    "            if Walls[s] == 1: continue\n",
    "            if Terminals[s] == 1:\n",
    "                V[s] = Rewards[s]\n",
    "                continue\n",
    "            Q = np.zeros((NA, 1))\n",
    "            for a in range(NA):\n",
    "                Q[a] = Rewards[s] + gamma * np.dot(T[s, :, a], V)\n",
    "            V[s] = np.max(Q)\n",
    "            Policy[s] = np.argmax(Q)\n",
    "        if np.sum(np.abs(V - V_old)) < 1e-10:\n",
    "            break\n",
    "    print(\"Value Iteration converged in\", iterations, \"iterations.\")\n",
    "    return V, Policy\n",
    "\n",
    "\n",
    "def policy_iteration(gamma, T, Rewards, Terminals, Walls):\n",
    "    S, NA = T.shape[0], T.shape[2]\n",
    "    V = np.zeros((N, 1))\n",
    "    Policy = np.zeros((N, 1))\n",
    "    iterations = 0\n",
    "\n",
    "    while True:\n",
    "        iterations += 1\n",
    "        policy_stable = True\n",
    "        b = Policy.copy()\n",
    "\n",
    "        # Policy Evaluation\n",
    "        while True:\n",
    "            delta = 0\n",
    "            for s in range(S):\n",
    "\n",
    "                if Terminals[s] == 1:\n",
    "                    V[s] = Rewards[s]\n",
    "                    continue\n",
    "                v = V[s].copy()\n",
    "                pi = int(Policy[s])\n",
    "                V[s] = Rewards[s] + gamma * np.dot(T[s, :, pi], V)\n",
    "                delta = max(delta, np.abs(v - V[s]))\n",
    "            if delta < 1e-1:\n",
    "                break\n",
    "\n",
    "        # Policy Iteration\n",
    "        for s in range(S):\n",
    "            if Terminals[s] == 1:\n",
    "                V[s] = Rewards[s]\n",
    "                continue\n",
    "            Q = np.zeros((NA, 1))\n",
    "\n",
    "            for a in range(NA):\n",
    "                Q[a] = Rewards[s] + gamma * np.dot(T[s, :, a], V)\n",
    "            Policy[s] = np.argmax(Q)\n",
    "\n",
    "            if b[s] != Policy[s]:\n",
    "                policy_stable = False\n",
    "        if policy_stable:\n",
    "            break\n",
    "    print(\"Policy Iteration converged in\", iterations, \"iterations.\")\n",
    "    return V, Policy\n",
    "\n",
    "\n",
    "def Q_learning(N_episodes, epsilon, alpha, gamma, T, Rewards, Terminals, Walls):\n",
    "    N, NA = T.shape[0], T.shape[2]\n",
    "    Q = np.zeros((N, NA))\n",
    "    for e in range(N_episodes):\n",
    "        s = int(np.floor(np.random.uniform(0, N - 1)))\n",
    "        while Terminals[s] == 1 or Walls[s] == 1:\n",
    "            s = int(np.floor(np.random.uniform(0, N - 1)))\n",
    "        while Terminals[s] == 0:\n",
    "            u = np.random.uniform(0, 1)\n",
    "            if u < epsilon:\n",
    "                a = int(np.floor(np.random.uniform(0, NA)))\n",
    "            else:\n",
    "                a = np.argmax(Q[s, :])\n",
    "            u = np.random.uniform(0, 1)\n",
    "            s1 = np.argmax(u < np.cumsum(T[s, :, a]))\n",
    "            Q[s, a] += alpha * (Rewards[s1] + gamma * np.max(Q[s1, :]) - Q[s, a])\n",
    "            s = s1\n",
    "        epsilon = epsilon * 0.9999  # Modify here for other annealing regimes\n",
    "        alpha = alpha * 0.9999  # Modify here for other annealing regimes\n",
    "    V = np.max(Q, axis=1)\n",
    "    V = V[:, None]\n",
    "    V[Terminals == 1] = Rewards[Terminals == 1]\n",
    "    Policy = np.array(np.argmax(Q, axis=1)).reshape((N, 1))\n",
    "    return V, Policy\n",
    "\n",
    "\n",
    "def TD_learning(N_episodes, alpha, gamma, T, Rewards, Terminals, Walls):\n",
    "    N, NA = T.shape[0], T.shape[2]\n",
    "    V = np.zeros((N, 1))\n",
    "    Policy = np.zeros((N, 1))\n",
    "    for episode in range(N_episodes):\n",
    "        s = int(np.floor(np.random.uniform(0, N - 1)))\n",
    "        while Terminals[s] == 1 or Walls[s] == 1:\n",
    "            s = int(np.floor(np.random.uniform(0, N - 1)))\n",
    "        while Terminals[s] == 0:\n",
    "            Q = np.zeros((NA, 1))\n",
    "            for a in range(NA):\n",
    "                Q[a] = Rewards[s] + gamma * np.dot(T[s, :, a], V)\n",
    "            a = np.argmax(Q)\n",
    "            u = np.random.uniform(0, 1)\n",
    "            s1 = np.argmax(u < np.cumsum(T[s, :, a]))\n",
    "            V[s] += alpha * (Rewards[s1] + gamma * V[s1] - V[s])\n",
    "            Policy[s] = a\n",
    "            s = s1\n",
    "            alpha = alpha * 0.9999\n",
    "\n",
    "    V[Terminals == 1] = Rewards[Terminals == 1]\n",
    "    return V, Policy\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class='alert alert-block alert-success' style=\"font-weight:bolder\">\n",
    "    \n",
    "## Task 3 - Experiments\n",
    "\n",
    "For the three grids you have, perform the following analysis and include them in your report,.\n",
    "\n",
    "- Compare value and policy iterations in terms of convergence time. Relate it to computational complexity, current implementation and number of iterations needed.\n",
    "\n",
    "- How are optimal policies change with immediate reward values? Show some examples (similar to Figure 17.2b in AIMA)\n",
    "\n",
    "- Compare TD-Learning and Q-Learning results with each other. Also with value and policy iterations. Remember that value and policy iteration solve Markov Decision Processes where we know the model (T and Rewards). TD-Learning and Q-Learning are (passive and active, respectively) Reinforcement Learning methods that don't have the model but use data from simulations. Data are simulated from the model we know, but the model is not used n TD or Q-Learning.\n",
    "\n",
    "- What are the effects of epsilon and alpha values and how they are modified?\n",
    "\n",
    "- What is the effect of number of episodes?\n",
    "    \n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Value Iteration converged in 29 iterations.\n",
      "Policy Iteration converged in 6 iterations.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/gq/2nwzl2_x23bg470lkxq503g40000gp/T/ipykernel_83720/1054134247.py:49: DeprecationWarning: Conversion of an array with ndim > 0 to a scalar is deprecated, and will error in future. Ensure you extract a single element from your array before performing this operation. (Deprecated NumPy 1.25.)\n",
      "  pi = int(Policy[s])\n",
      "/var/folders/gq/2nwzl2_x23bg470lkxq503g40000gp/T/ipykernel_83720/1054134247.py:89: DeprecationWarning: Conversion of an array with ndim > 0 to a scalar is deprecated, and will error in future. Ensure you extract a single element from your array before performing this operation. (Deprecated NumPy 1.25.)\n",
      "  Q[s, a] += alpha * (Rewards[s1] + gamma * np.max(Q[s1, :]) - Q[s, a])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Value Iteration:\n",
      " [[ 0.81155822  0.86780822  0.91780822  1.        ]\n",
      " [ 0.76155822  0.          0.66027397 -1.        ]\n",
      " [ 0.70530822  0.65530822  0.61141553  0.38792491]]\n",
      "Maze reshape:\n",
      " [['R' 'R' 'R' 'G']\n",
      " ['U' 'W' 'U' 'G']\n",
      " ['U' 'L' 'L' 'L']]\n",
      "\n",
      "Policy Iteration:\n",
      " [[ 0.81086593  0.86780445  0.917808    1.        ]\n",
      " [ 0.76007897  0.79627873  0.66027367 -1.        ]\n",
      " [ 0.689108    0.61655665  0.58556575  0.36414374]]\n",
      "Maze reshape:\n",
      " [['R' 'R' 'R' 'G']\n",
      " ['U' 'W' 'U' 'G']\n",
      " ['U' 'L' 'U' 'L']]\n",
      "\n",
      "Q Learning:\n",
      " [[ 0.88661998  0.94603854  0.99689938  1.        ]\n",
      " [ 0.81379897  0.          0.63040612 -1.        ]\n",
      " [ 0.75814308  0.69969592  0.66335169  0.54054948]]\n",
      "Maze reshape:\n",
      " [['R' 'R' 'R' 'G']\n",
      " ['U' 'W' 'L' 'G']\n",
      " ['U' 'L' 'L' 'L']]\n",
      "\n",
      "TD Learning:\n",
      " [[ 0.42489962  0.52568713  0.57742369  1.        ]\n",
      " [ 0.30058905  0.          0.26359621 -1.        ]\n",
      " [ 0.11421557 -0.02617961 -0.00857958 -0.88289877]]\n",
      "Maze reshape:\n",
      " [['R' 'R' 'U' 'G']\n",
      " ['U' 'W' 'U' 'G']\n",
      " ['U' 'L' 'U' 'U']]\n"
     ]
    }
   ],
   "source": [
    "gamma = 1\n",
    "alpha = 0.9\n",
    "epsilon = 0.5\n",
    "N_episodes = 20000\n",
    "\n",
    "#Value iteration\n",
    "V_VI, Policy_VI = value_iteration(gamma, T, Rewards, Terminals, Walls)\n",
    "\n",
    "#Policy iteration\n",
    "V_PI, Policy_PI = policy_iteration(gamma, T, Rewards, Terminals, Walls)\n",
    "\n",
    "#Q-Learning\n",
    "V_Q, Policy_Q = Q_learning(N_episodes, epsilon, alpha, gamma, T, Rewards, Terminals, Walls)\n",
    "\n",
    "#TD-Learning\n",
    "V_TD, Policy_TD = TD_learning(N_episodes, alpha, gamma, T, Rewards, Terminals, Walls)\n",
    "\n",
    "print(\"Value Iteration:\\n\", V_VI.reshape((H, W)))\n",
    "maze = Directions[Policy_VI.astype(int)]\n",
    "maze[Walls == 1] = 'W'\n",
    "maze[Terminals == 1] = 'G'\n",
    "print(\"Maze reshape:\\n\", maze.reshape((H, W)))\n",
    "#\n",
    "print(\"\\nPolicy Iteration:\\n\", V_PI.reshape((H, W)))\n",
    "maze = Directions[Policy_PI.astype(int)]\n",
    "maze[Walls == 1] = 'W'\n",
    "maze[Terminals == 1] = 'G'\n",
    "print(\"Maze reshape:\\n\", maze.reshape((H, W)))\n",
    "#\n",
    "print(\"\\nQ Learning:\\n\", V_Q.reshape((H, W)))\n",
    "maze_Q = Directions[Policy_Q.astype(int)]\n",
    "maze_Q[Walls == 1] = 'W'\n",
    "maze_Q[Terminals == 1] = 'G'\n",
    "print(\"Maze reshape:\\n\", maze_Q.reshape((H, W)))\n",
    "#\n",
    "print(\"\\nTD Learning:\\n\", V_TD.reshape((H, W)))\n",
    "maze_TD = Directions[Policy_TD.astype(int)]\n",
    "maze_TD[Walls == 1] = 'W'\n",
    "maze_TD[Terminals == 1] = 'G'\n",
    "print(\"Maze reshape:\\n\", maze_TD.reshape((H, W)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class='alert alert-block alert-success' style=\"font-weight:bolder\">\n",
    "\n",
    "## Task 4 (extra credit) - What if diagonal moves were possible?\n",
    "\n",
    "Repeat the experiments in one of these settings. You don't have to stick to the code provided here. But, you can only use Numpy (and matplotlib).\n",
    "\n",
    "You should decide on how to distribute the noise associated with actions across (next) states. For example, in the scenario above we have probability of going Up with the Up action is 1-noise. The probability of going Left with the Up action is noise/2. The same for going Right.\n",
    "    \n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transition Probabilities:\n",
      "[[[0.8  0.   0.1  ... 0.   0.05 0.  ]\n",
      "  [0.   0.8  0.1  ... 0.05 0.   0.05]\n",
      "  [0.   0.   0.   ... 0.   0.   0.  ]\n",
      "  ...\n",
      "  [0.   0.   0.   ... 0.   0.   0.  ]\n",
      "  [0.   0.   0.   ... 0.   0.   0.  ]\n",
      "  [0.   0.   0.   ... 0.   0.   0.  ]]\n",
      "\n",
      " [[0.8  0.   0.1  ... 0.   0.05 0.  ]\n",
      "  [0.1  0.1  0.   ... 0.   0.05 0.05]\n",
      "  [0.   0.8  0.1  ... 0.05 0.   0.05]\n",
      "  ...\n",
      "  [0.   0.   0.   ... 0.   0.   0.  ]\n",
      "  [0.   0.   0.   ... 0.   0.   0.  ]\n",
      "  [0.   0.   0.   ... 0.   0.   0.  ]]\n",
      "\n",
      " [[0.   0.   0.   ... 0.   0.   0.  ]\n",
      "  [0.8  0.   0.1  ... 0.   0.05 0.  ]\n",
      "  [0.   0.   0.   ... 0.   0.   0.  ]\n",
      "  ...\n",
      "  [0.   0.   0.   ... 0.   0.   0.  ]\n",
      "  [0.   0.   0.   ... 0.   0.   0.  ]\n",
      "  [0.   0.   0.   ... 0.   0.   0.  ]]\n",
      "\n",
      " ...\n",
      "\n",
      " [[0.   0.   0.   ... 0.   0.   0.  ]\n",
      "  [0.   0.   0.   ... 0.   0.   0.  ]\n",
      "  [0.   0.   0.   ... 0.   0.   0.  ]\n",
      "  ...\n",
      "  [0.1  0.1  0.   ... 0.   0.05 0.05]\n",
      "  [0.   0.8  0.1  ... 0.05 0.   0.05]\n",
      "  [0.   0.   0.   ... 0.   0.   0.  ]]\n",
      "\n",
      " [[0.   0.   0.   ... 0.   0.   0.  ]\n",
      "  [0.   0.   0.   ... 0.   0.   0.  ]\n",
      "  [0.   0.   0.   ... 0.   0.   0.  ]\n",
      "  ...\n",
      "  [0.8  0.   0.1  ... 0.   0.05 0.  ]\n",
      "  [0.1  0.1  0.   ... 0.   0.05 0.05]\n",
      "  [0.   0.8  0.1  ... 0.05 0.   0.05]]\n",
      "\n",
      " [[0.   0.   0.   ... 0.   0.   0.  ]\n",
      "  [0.   0.   0.   ... 0.   0.   0.  ]\n",
      "  [0.   0.   0.   ... 0.   0.   0.  ]\n",
      "  ...\n",
      "  [0.   0.   0.   ... 0.   0.   0.  ]\n",
      "  [0.8  0.   0.1  ... 0.   0.05 0.  ]\n",
      "  [0.1  0.9  0.1  ... 0.05 0.05 0.1 ]]]\n",
      "\n",
      "Rewards:\n",
      "[[-0.04]\n",
      " [-0.04]\n",
      " [-0.04]\n",
      " [ 1.  ]\n",
      " [-0.04]\n",
      " [-0.04]\n",
      " [-0.04]\n",
      " [-1.  ]\n",
      " [-0.04]\n",
      " [-0.04]\n",
      " [-0.04]\n",
      " [-0.04]]\n",
      "\n",
      "Terminal States:\n",
      "[[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "\n",
      "Walls:\n",
      "[[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "\n",
      "Directions:\n",
      "['L' 'R' 'U' 'D' 'UL' 'UR' 'DL' 'DR']\n",
      "\n",
      "Grid Dimensions (W, H): 4 3\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "def read_grid(file_path):\n",
    "    file1 = open(file_path, 'r')\n",
    "\n",
    "    W, H = [t(s) for t, s in zip((int, int), file1.readline().split())]\n",
    "    noise = [t(s) for t, s in zip((float, str), file1.readline().split())][0]\n",
    "    immediate_rewards = [t(s) for t, s in zip((float, str), file1.readline().split())][0]\n",
    "    N = W * H  # total number of states\n",
    "    NA = 8  # number of actions (including diagonals)\n",
    "    T = np.zeros((N, N, NA))  # state transition probabilities\n",
    "    Directions = np.array(['L', 'R', 'U', 'D', 'UL', 'UR', 'DL', 'DR'])\n",
    "    Rewards = np.zeros((N, 1)) + immediate_rewards  # immediate rewards\n",
    "    Terminals = np.zeros((N, 1))  # terminal states\n",
    "    Walls = np.zeros((N, 1))  # (internal) wall states\n",
    "    prob1 = 1 - noise\n",
    "    prob2 = noise / 2\n",
    "\n",
    "    while True:\n",
    "        line = file1.readline()\n",
    "        if not line:\n",
    "            break\n",
    "        if line.find('T') != -1:\n",
    "            (_, x, y, r) = [t(s) for t, s in zip((str, int, int, float), line.split())]\n",
    "            Terminals[y * W + x] = 1\n",
    "            Rewards[y * W + x] = r\n",
    "        if line.find('W') != -1:\n",
    "            (_, x, y) = [t(s) for t, s in zip((str, int, int), line.split())]\n",
    "            Walls[y * W + x] = 1\n",
    "\n",
    "    file1.close()\n",
    "\n",
    "    for i in range(W):\n",
    "        for j in range(H):\n",
    "            # Actions: 0-L, 1-R, 2-U, 3-D, 4-UL, 5-UR, 6-DL, 7-DR\n",
    "            if i + 1 < W and Walls[j * W + i + 1] == 0:  # not wall on the right\n",
    "                T[j * W + i, j * W + i + 1, 1] += prob1\n",
    "                T[j * W + i, j * W + i + 1, 2] += prob2\n",
    "                T[j * W + i, j * W + i + 1, 3] += prob2\n",
    "                T[j * W + i, j * W + i + 1, 5] += prob2 / 2  # UR\n",
    "                T[j * W + i, j * W + i + 1, 7] += prob2 / 2  # DR\n",
    "            else:\n",
    "                T[j * W + i, j * W + i, 1] += prob1\n",
    "                T[j * W + i, j * W + i, 2] += prob2\n",
    "                T[j * W + i, j * W + i, 3] += prob2\n",
    "                T[j * W + i, j * W + i, 5] += prob2 / 2  # UR\n",
    "                T[j * W + i, j * W + i, 7] += prob2 / 2  # DR\n",
    "\n",
    "            if i - 1 >= 0 and Walls[j * W + i - 1] == 0:  # not wall on the left\n",
    "                T[j * W + i, j * W + i - 1, 0] += prob1\n",
    "                T[j * W + i, j * W + i - 1, 2] += prob2\n",
    "                T[j * W + i, j * W + i - 1, 3] += prob2\n",
    "                T[j * W + i, j * W + i - 1, 4] += prob2 / 2  # UL\n",
    "                T[j * W + i, j * W + i - 1, 6] += prob2 / 2  # DL\n",
    "            else:\n",
    "                T[j * W + i, j * W + i, 0] += prob1\n",
    "                T[j * W + i, j * W + i, 2] += prob2\n",
    "                T[j * W + i, j * W + i, 3] += prob2\n",
    "                T[j * W + i, j * W + i, 4] += prob2 / 2  # UL\n",
    "                T[j * W + i, j * W + i, 6] += prob2 / 2  # DL\n",
    "\n",
    "            if j + 1 < H and Walls[(j + 1) * W + i] == 0:  # not wall below\n",
    "                T[j * W + i, (j + 1) * W + i, 3] += prob1\n",
    "                T[j * W + i, (j + 1) * W + i, 0] += prob2\n",
    "                T[j * W + i, (j + 1) * W + i, 1] += prob2\n",
    "                T[j * W + i, (j + 1) * W + i, 6] += prob2 / 2  # DL\n",
    "                T[j * W + i, (j + 1) * W + i, 7] += prob2 / 2  # DR\n",
    "            else:\n",
    "                T[j * W + i, j * W + i, 3] += prob1\n",
    "                T[j * W + i, j * W + i, 0] += prob2\n",
    "                T[j * W + i, j * W + i, 1] += prob2\n",
    "                T[j * W + i, j * W + i, 6] += prob2 / 2  # DL\n",
    "                T[j * W + i, j * W + i, 7] += prob2 / 2  # DR\n",
    "\n",
    "            if i + 1 < W and j - 1 >= 0 and Walls[(j - 1) * W + i + 1] == 0:  # not wall on the down-right\n",
    "                T[j * W + i, (j - 1) * W + i + 1, 5] += prob2 / 2  # Up-Right\n",
    "                T[j * W + i, (j - 1) * W + i + 1, 7] += prob2 / 2  # Down-Right\n",
    "\n",
    "            if i - 1 >= 0 and j - 1 >= 0 and Walls[(j - 1) * W + i - 1] == 0:  # not wall on the down-left\n",
    "                T[j * W + i, (j - 1) * W + i - 1, 4] += prob2 / 2  # Up-Left\n",
    "                T[j * W + i, (j - 1) * W + i - 1, 6] += prob2 / 2  # Down-Left\n",
    "\n",
    "    return T, Rewards, Terminals, Walls, Directions, W, H\n",
    "\n",
    "# Example usage\n",
    "file_path = 'grid1.txt'\n",
    "T, Rewards, Terminals, Walls, Directions, W, H = read_grid(file_path)\n",
    "\n",
    "print(\"Transition Probabilities:\")\n",
    "print(T)\n",
    "print(\"\\nRewards:\")\n",
    "print(Rewards)\n",
    "print(\"\\nTerminal States:\")\n",
    "print(Terminals)\n",
    "print(\"\\nWalls:\")\n",
    "print(Walls)\n",
    "print(\"\\nDirections:\")\n",
    "print(Directions)\n",
    "print(\"\\nGrid Dimensions (W, H):\", W, H)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
