{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tabulate in ./venv/lib/python3.10/site-packages (0.9.0)\r\n",
      "Requirement already satisfied: sklearn in ./venv/lib/python3.10/site-packages (0.0)\r\n",
      "Requirement already satisfied: scikit-learn in ./venv/lib/python3.10/site-packages (from sklearn) (1.3.2)\r\n",
      "Requirement already satisfied: numpy<2.0,>=1.17.3 in ./venv/lib/python3.10/site-packages (from scikit-learn->sklearn) (1.26.2)\r\n",
      "Requirement already satisfied: scipy>=1.5.0 in ./venv/lib/python3.10/site-packages (from scikit-learn->sklearn) (1.11.4)\r\n",
      "Requirement already satisfied: joblib>=1.1.1 in ./venv/lib/python3.10/site-packages (from scikit-learn->sklearn) (1.3.2)\r\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in ./venv/lib/python3.10/site-packages (from scikit-learn->sklearn) (3.2.0)\r\n"
     ]
    }
   ],
   "source": [
    "!pip install tabulate sklearn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part B: Naive Bayes Classifier\n",
    "\n",
    "This notebook will introduce you to the basics of the Naive Bayes Algorithm for classification tasks. It includes the following content:\n",
    "\n",
    "- A brief overview of the Naive Bayes (NB) Classifier\n",
    "- An example exercise of performing inference with NB\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What is a classifier?\n",
    "\n",
    "A classifier is a machine learning model that is used to discriminate different objects based on specific features. Given sample data $X$, a classifier predicts the class $y$ it belongs to.\n",
    "\n",
    "## What is a Naive Bayes Classifier?\n",
    "\n",
    "A Naive Bayes classifier is a probabilistic machine learning model for solving classification tasks. It is based on Bayes theorem and imposes a strong assumption on feature independence.\n",
    "\n",
    "## Bayes Theorem\n",
    "\n",
    "$$ P(A \\mid B) = \\frac{P(B \\mid A) \\, P(A)}{P(B)} $$\n",
    "\n",
    "We can compute the probability of event A happening, given the fact that event B has occurred. Event B is the evidence and event A is the hypothesis. The assumption made by Naive Bayes is that the features are independent, i.e. the presence of one feature does not affect the other. Therefore it is called naive.\n",
    "\n",
    "Under the context of classification tasks, given the observation $X$, the classifier casts prediction on the class $y$. It can also be rewritten (with $y$ and $X$ replacing $A$ and $B$) as\n",
    "\n",
    "$$ P(y \\mid X) = \\frac{P(X \\mid y) \\, P(y)}{P(X)} $$\n",
    "\n",
    "The formula consists of four components:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- $\n",
    "P(y \\mid X) :\n",
    "\\:$ The posterior probability, which is the probability of class $y$ given the observation $X$\n",
    "\n",
    "- $\n",
    "P(y) :\n",
    "\\:$ The Prior probability, which is the prior probability (initial belief) of class $y$\n",
    "\n",
    "- $\n",
    "P(X \\mid y) :\n",
    "\\:$The Likelihood, which is the probability of observation $X$ given class $y$.\n",
    "\n",
    "- $\n",
    "P(X) :\n",
    "\\:$The Evidence, which is the probability of observation $X$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In classification tasks, the variable $y$ is the class label. The variable X represents the parameters/features and it usually contains multiple features/dimensions:\n",
    "\n",
    "$$ X = (x_1, x_2, x_3, ..., x_n) $$\n",
    "\n",
    "where $x_1, x_2, ..., x_n$ are the features and they are assumed to be independent in NB, i.e. $ (\\:x_i \\: \\bot \\:  x_j \\mid y)\\:\\: \\text{for all features}$ ($i \\neq j$ and $i, j \\in \\{1, 2, ...., n\\}$). By expanding using the chain rule we obtained the following:\n",
    "\n",
    "$$ P(y \\mid x_1, x_2, ..., x_n) = \\frac{P(x_1, x_2, ..., x_n \\mid y) \\, P(y)}{P(X)} = \\frac{P(x_1 \\mid y) P(x_2 \\mid y) P(x_3 \\mid y) \\cdots P(x_n \\mid y) \\, P(y)}{P(x_1) P(x_2) P(x_3) \\cdots P(x_n)} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The denominator ($P(X)$) of the Bayes rule remains the same for all classes. Therefore, we can exclude it when performing inference since it is just a term for normalization. Therefore, based on the assumption of feature independence and ignoring the denominator the NB formula can be written as follows:\n",
    "\n",
    "$$ P(\\: y \\mid x_1,x_2,...,x_n)\\: \\propto P(y) \\prod_{i=1}^{i=n} P(\\:x_i\\mid y) $$\n",
    "\n",
    "In (binary) classification tasks, the class variable $y$ has two outcomes. We need to find the class $y$ with maximum probability, i.e. $ y = argmax_y P(y) \\prod_{i=1}^{i=n} P(\\:x_i\\mid y) $."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## An example exercise of performing inference with NB"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use the following example to strengthen our understanding of NB. The example toy dataset is for classifying whether a person owns a pet. Observations $X$ contain three features, two categorical (\"Gender\" and \"Education\") and one numerical (\"Income\"), and class label $y$ (i.e. \"Has_pet\") corresponds to whether this person owns a pet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<tbody>\n",
       "<tr><td>Female</td><td>University</td><td style=\"text-align: right;\">103000</td><td>Yes</td></tr>\n",
       "<tr><td>Female</td><td>HighSchool</td><td style=\"text-align: right;\"> 90500</td><td>No </td></tr>\n",
       "<tr><td>Female</td><td>HighSchool</td><td style=\"text-align: right;\">114000</td><td>No </td></tr>\n",
       "<tr><td>Male  </td><td>University</td><td style=\"text-align: right;\">102000</td><td>No </td></tr>\n",
       "<tr><td>Male  </td><td>University</td><td style=\"text-align: right;\"> 75000</td><td>Yes</td></tr>\n",
       "<tr><td>Male  </td><td>HighSchool</td><td style=\"text-align: right;\"> 90000</td><td>No </td></tr>\n",
       "<tr><td>Male  </td><td>HighSchool</td><td style=\"text-align: right;\"> 85000</td><td>Yes</td></tr>\n",
       "<tr><td>Male  </td><td>University</td><td style=\"text-align: right;\"> 86000</td><td>No </td></tr>\n",
       "</tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import HTML, display\n",
    "import tabulate\n",
    "import pandas as pd\n",
    "\n",
    "tab_cat = [[\"Female\", \"University\", 103000,   \"Yes\"],\n",
    "          [\"Female\", \"HighSchool\", 90500,   \"No\"],\n",
    "          [\"Female\", \"HighSchool\", 114000,   \"No\"],\n",
    "          [\"Male\",   \"University\", 102000,   \"No\"],\n",
    "          [\"Male\",   \"University\", 75000,   \"Yes\"],\n",
    "          [\"Male\",   \"HighSchool\", 90000,   \"No\"],\n",
    "          [\"Male\",   \"HighSchool\", 85000,   \"Yes\"],\n",
    "          [\"Male\",   \"University\", 86000,   \"No\"]]\n",
    "\n",
    "display(HTML(tabulate.tabulate(tab_cat, tablefmt='html')))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "<div class='alert alert-block alert-success' style=\"font-weight:bolder\">\n",
    "\n",
    "### Task 2a - Compute the Likelihood table of having pet, for each categorical feature, as well as the marginal probability.\n",
    "\n",
    "- $P(Gender|Has\\_pet)$: $P(Male|Yes)$, $P(Female|Yes)$, $P(Male|No)$, $P(Female|No)$\n",
    "    \n",
    "- $P(Education|Has\\_pet)$: $P(University|Yes)$, $P(HighSchool|Yes)$, $P(University|No)$, $P(HighSchool|No)$\n",
    "    \n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<tbody>\n",
       "<tr><td>likelihood</td><td>-         </td><td>Has_pet</td><td>-    </td><td>-        </td></tr>\n",
       "<tr><td>-         </td><td>-         </td><td>Yes    </td><td>No   </td><td>P(Gender)</td></tr>\n",
       "<tr><td>Gender    </td><td>Male      </td><td>0.67   </td><td>0.6  </td><td>0.625    </td></tr>\n",
       "<tr><td>-         </td><td>Female    </td><td>0.33   </td><td>0.4  </td><td>0.375    </td></tr>\n",
       "<tr><td>-         </td><td>P(Has_pet)</td><td>0.375  </td><td>0.625</td><td>         </td></tr>\n",
       "</tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<tbody>\n",
       "<tr><td>likelihood</td><td>-         </td><td>Has_pet</td><td>-    </td><td>-           </td></tr>\n",
       "<tr><td>-         </td><td>-         </td><td>Yes    </td><td>No   </td><td>P(Education)</td></tr>\n",
       "<tr><td>Education </td><td>University</td><td>0.67   </td><td>0.4  </td><td>0.5         </td></tr>\n",
       "<tr><td>-         </td><td>HighSchool</td><td>0.33   </td><td>0.6  </td><td>0.5         </td></tr>\n",
       "<tr><td>-         </td><td>P(Has_pet)</td><td>0.375  </td><td>0.625</td><td>            </td></tr>\n",
       "</tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "df = pd.DataFrame(tab_cat, columns=[\"Gender\", \"Education\", \"Income\", \"Has_pet\"])\n",
    "\n",
    "\n",
    "def probability_gender(df, has_pet, gender):\n",
    "    # Get number of elements in the dataframe\n",
    "    nb_elements = df.shape[0]\n",
    "\n",
    "    # Get the % of element that has or has not a pet (divided by the number of elements in the df)\n",
    "    nb_has_pet = len(df[df.Has_pet == has_pet]) / nb_elements\n",
    "\n",
    "    # Get the % of the given gender (divided by the number of elements in the df)\n",
    "    nb_gender = len(df[df.Gender == gender]) / nb_elements\n",
    "\n",
    "    # Get the % of gender who has or has not a pet (divided by the total number of the same gender)\n",
    "    # P(has_pet|Gender)\n",
    "    nb_pet_owners_who_are_gender = len(df[(df['Gender'] == gender) & (df['Has_pet'] == has_pet)]) / len(df[df.Gender == gender])\n",
    "\n",
    "    return round((nb_pet_owners_who_are_gender * nb_gender) / nb_has_pet, 2)\n",
    "\n",
    "\n",
    "marginal_gender = round(df['Gender'].value_counts() / df.shape[0], 3)\n",
    "\n",
    "marginal_has_pets = round(df['Has_pet'].value_counts() / df.shape[0], 3)\n",
    "\n",
    "tab_likelihood_gender = [\n",
    "    [\"likelihood\", \"-\", \"Has_pet\", \"-\", \"-\"],\n",
    "    [\"-\", \"-\", \"Yes\", \"No\", \"P(Gender)\"],\n",
    "    [\"Gender\", \"Male\", probability_gender(df, \"Yes\", \"Male\"), probability_gender(df, \"No\", \"Male\"), marginal_gender[\"Male\"]],\n",
    "    [\"-\", \"Female\", probability_gender(df, \"Yes\", \"Female\"), probability_gender(df, \"No\", \"Female\"), marginal_gender[\"Female\"]],\n",
    "    [\"-\", \"P(Has_pet)\", marginal_has_pets[\"Yes\"], marginal_has_pets[\"No\"], \"\"]\n",
    "]\n",
    "display(HTML(tabulate.tabulate(tab_likelihood_gender, tablefmt='html')))\n",
    "\n",
    "\n",
    "def probability_education(df, has_pet, education):\n",
    "    # Get number of elements in the dataframe\n",
    "    nb_elements = df.shape[0]\n",
    "\n",
    "    # Get the % of element that has or has not a pet (divided by the number of elements in the df)\n",
    "    nb_has_pet = len(df[df.Has_pet == has_pet]) / nb_elements\n",
    "\n",
    "    # Get the % of the given education (divided by the number of elements in the df)\n",
    "    nb_education = len(df[df.Education == education]) / nb_elements\n",
    "\n",
    "    # Get the % of education who has or has not a pet (divided by the total number of the same gender)\n",
    "    nb_pet_owners_who_are_education = len(df[(df['Education'] == education) & (df['Has_pet'] == has_pet)]) / len(df[df.Education == education])\n",
    "\n",
    "    return round((nb_pet_owners_who_are_education * nb_education) / nb_has_pet, 2)\n",
    "\n",
    "\n",
    "marginal_education = round(df['Education'].value_counts() / df.shape[0], 3)\n",
    "\n",
    "tab_likelihood_gender = [\n",
    "    [\"likelihood\", \"-\", \"Has_pet\", \"-\", \"-\"],\n",
    "    [\"-\", \"-\", \"Yes\", \"No\", \"P(Education)\"],\n",
    "    [\"Education\", \"University\", probability_education(df, \"Yes\", \"University\"),\n",
    "     probability_education(df, \"No\", \"University\"), marginal_education[\"University\"]],\n",
    "    [\"-\", \"HighSchool\", probability_education(df, \"Yes\", \"HighSchool\"), probability_education(df, \"No\", \"HighSchool\"),\n",
    "     marginal_education[\"HighSchool\"]],\n",
    "    [\"-\", \"P(Has_pet)\", marginal_has_pets[\"Yes\"], marginal_has_pets[\"No\"], \"\"]\n",
    "]\n",
    "display(HTML(tabulate.tabulate(tab_likelihood_gender, tablefmt='html')))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class='alert alert-block alert-success' style=\"font-weight:bolder\">\n",
    "\n",
    "### Task 2b - Compute the posterior probability\n",
    "\n",
    "- $P(\\text{No}|\\text{Male})$, $P(\\text{Yes}|\\text{Female})$\n",
    "    \n",
    "- $P(\\text{Yes}|\\text{Univeristy})$, $P(\\text{No}|\\text{HighSchool})$\n",
    "\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Posterior probability for P(No|Male): 0.6\n",
      "Posterior probability for P(Yes|Female): 0.33\n",
      "Posterior probability for P(No|HighSchool): 0.75\n",
      "Posterior probability for P(Yes|University): 0.5\n"
     ]
    }
   ],
   "source": [
    "def posterior_probability(df, is_gender, feature, has_pets):\n",
    "    if is_gender:\n",
    "        nb_pet_owners_who_are_gender = len(df[(df['Gender'] == feature) & (df['Has_pet'] == has_pets)]) / len(df[df.Gender == feature])\n",
    "        return round(nb_pet_owners_who_are_gender, 2)\n",
    "    else:\n",
    "        nb_pet_owners_who_are_education = len(df[(df['Education'] == feature) & (df['Has_pet'] == has_pets)]) / len(df[df.Education == feature])\n",
    "        return round(nb_pet_owners_who_are_education, 2)\n",
    "\n",
    "\n",
    "# P(No | Male)\n",
    "print(\"Posterior probability for P(No|Male):\", (posterior_probability(df, True, \"Male\", \"No\")))\n",
    "# P(Yes | Female)\n",
    "print(\"Posterior probability for P(Yes|Female):\", posterior_probability(df, True, \"Female\", \"Yes\"))\n",
    "# P(No | University)\n",
    "print(\"Posterior probability for P(No|HighSchool):\", posterior_probability(df, False, \"HighSchool\", \"No\"))\n",
    "# P(Yes | University)\n",
    "print(\"Posterior probability for P(Yes|University):\", posterior_probability(df, False, \"University\", \"Yes\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class='alert alert-block alert-success' style=\"font-weight:bolder\">\n",
    "\n",
    "### Task 2c - Compute the Likelihood of having pets using mean, standard deviation, and normal distribution function:\n",
    "\n",
    "- Mean: $ \\mu = \\frac{1}{n} \\sum^{n}_{i=1}{x_i} $\n",
    "    \n",
    "- Standard Deviation $ \\sigma = \\left[ \\frac{1}{n-1} \\sum^{n}_{i=1}{(x_i-\\mu)^2} \\right]^\\frac{1}{2}  $\n",
    "    \n",
    "- Normal Distribution $f(x)=\\dfrac{1}{\\sigma\\sqrt{2\\pi}}\\,e^{-\\dfrac{(x-\\mu)^2}{2\\sigma{}^2}}$\n",
    "    \n",
    "Compute $L( \\text{Income}=90000 \\mid \\text{Yes})$, $L( \\text{Income}=90000 \\mid \\text{No})$\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Have pets = Mean:    0.375 ; Standard deviation:    0.518\n",
      "Normal Distribution for having a pet:        0.372\n",
      "Normal Distribution for not having a pet:    0.593\n",
      "\n",
      "L(Income = 90000 | Yes): 0.3333333333333333\n",
      "L(Income = 90000 | No): 0.2\n"
     ]
    }
   ],
   "source": [
    "from scipy.stats import norm\n",
    "\n",
    "income = 90000\n",
    "\n",
    "having_pets_mean = df['Has_pet'].map({'Yes': 1, 'No': 0}).mean()\n",
    "\n",
    "# Standard Deviation\n",
    "having_pets_std_dev = (df['Has_pet'].map({'Yes': 1, 'No': 0})).std()\n",
    "\n",
    "# Normal distribution\n",
    "normal_distribution = norm(having_pets_mean, having_pets_std_dev)\n",
    "\n",
    "probability_has_pet = normal_distribution.pdf(1)  # Probability of having a pet (Yes)\n",
    "probability_no_pet = normal_distribution.pdf(0)  # Probability of not having a pet (No)\n",
    "\n",
    "print(f\"Have pets = Mean: {having_pets_mean:>8.3f} ; Standard deviation: {having_pets_std_dev:>8.3f}\")\n",
    "print(f\"Normal Distribution for having a pet: {probability_has_pet:>12.3f}\")\n",
    "print(f\"Normal Distribution for not having a pet: {probability_no_pet:>8.3f}\\n\")\n",
    "\n",
    "nb_income = len(df[df.Income == income])\n",
    "\n",
    "L_inc_yes = nb_income / len(df[df.Has_pet == \"Yes\"])\n",
    "L_inc_no = nb_income / len(df[df.Has_pet == \"No\"])\n",
    "\n",
    "print(\"L(Income = 90000 | Yes):\", L_inc_yes)\n",
    "print(\"L(Income = 90000 | No):\", L_inc_no)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class='alert alert-block alert-success' style=\"font-weight:bolder\">\n",
    "\n",
    "### Task 2d - Making inference for the given examples\n",
    "\n",
    "- $X=(Education=University, Gender=Female, Income=100000)$\n",
    "    \n",
    "- $X=(Education=HighSchool, Gender=Male, Income=92000)$\n",
    "\n",
    "</div>\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<tbody>\n",
       "<tr><td>       </td><td>   </td><td>X1                    </td><td>X2                    </td></tr>\n",
       "<tr><td>       </td><td>   </td><td>University            </td><td>HighSchool            </td></tr>\n",
       "<tr><td>       </td><td>   </td><td>Female                </td><td>Male                  </td></tr>\n",
       "<tr><td>       </td><td>   </td><td>100,000               </td><td>92,000                </td></tr>\n",
       "<tr><td>       </td><td>   </td><td>_______________       </td><td>_____________         </td></tr>\n",
       "<tr><td>Has_Pet</td><td>Yes</td><td>1.065450647621758e-06 </td><td>6.948597222460778e-07 </td></tr>\n",
       "<tr><td>       </td><td>No </td><td>1.2850301795528517e-06</td><td>1.8856437510069952e-06</td></tr>\n",
       "</tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import math\n",
    "\n",
    "def probability_income(df, income, df_income):\n",
    "    count = len(df_income)\n",
    "    mu = (1/len(df)) * count\n",
    "\n",
    "    sommation = 0.0\n",
    "    for i in range(count) :\n",
    "        sommation = sommation + (income - mu)**2\n",
    "\n",
    "    sig = ((1/(len(df)-1)) * sommation)**(1/2)\n",
    "\n",
    "    exp = math.exp(- ((income - mu)**2) / (2 * (sig**2) ))\n",
    "    square = math.sqrt(2 * math.pi)\n",
    "    result =  (1/(sig * square))*exp\n",
    "    return result\n",
    "\n",
    "\n",
    "def inference(df, has_pet, gender, education, income):\n",
    "    df_income = df[\"Income\"]\n",
    "\n",
    "    P_gender_has_pet = probability_gender(df, has_pet, gender)\n",
    "    P_education_has_pet = probability_education(df, has_pet, education)\n",
    "    P_income_has_pet = probability_income(df, income, df_income)\n",
    "\n",
    "    P_gender = round(len(df[df.Gender == gender]) / df.shape[0], 3)\n",
    "    P_education = round(len(df[df.Education == education]) / df.shape[0], 3)\n",
    "    P_income = len(df_income) / len(df)\n",
    "\n",
    "    having_pets_mean = df['Has_pet'].map({'Yes': 1, 'No': 0}).mean()\n",
    "\n",
    "    P_X = (P_gender_has_pet * P_education_has_pet * P_income_has_pet * marginal_has_pets[has_pet]) / (P_gender * P_education * P_income)\n",
    "\n",
    "    return P_X\n",
    "\n",
    "P_has_pet_1 = inference(df, \"Yes\", \"Female\", \"University\", 100000)\n",
    "P_has_no_pet_1 = inference(df, \"No\", \"Female\", \"University\", 100000)\n",
    "P_has_pet_2 = inference(df, \"Yes\", \"Male\", \"HighSchool\", 92000)\n",
    "P_has_no_pet_2 = inference(df, \"No\", \"Male\", \"HighSchool\", 92000)\n",
    "\n",
    "tab_Has_pet = [\n",
    "    [\"\",\"\",  \"X1\", \"X2\"],\n",
    "    [\"\",\"\",  \"University\", \"HighSchool\"],\n",
    "    [\"\",\"\",  \"Female\",  \"Male\"],\n",
    "    [\"\",\"\",  \"100,000\", \"92,000\"],\n",
    "    [\"\",\"\",  \"_______________\", \"_____________\"],\n",
    "    [\"Has_Pet\", \"Yes\",P_has_pet_1 ,P_has_pet_2],\n",
    "    [\"\", \"No\", P_has_no_pet_1, P_has_no_pet_2]\n",
    "]\n",
    "display(HTML(tabulate.tabulate(tab_Has_pet, tablefmt='html')))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class='alert alert-block alert-success' style=\"font-weight:bolder\">\n",
    "\n",
    "### Task 2e (Extra Credit) Implementing a Naive Bayes Classifier and performing classification on the Iris dataset. Note that the Iris dataset only contains numerical features.\n",
    "\n",
    "</div>\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "\n",
    "iris = load_iris()\n",
    "X, y = iris[\"data\"], iris[\"target\"]\n",
    "print(\"data\", X)\n",
    "print(\"class/label\", y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "vscode": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
